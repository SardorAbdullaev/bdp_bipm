{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%cd ~/bdp_bipm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os, json, sys\n",
    "import shutil\n",
    "import multiprocessing\n",
    "import copy\n",
    "from glob import glob\n",
    "import urllib\n",
    "import pickle\n",
    "import urllib.request\n",
    "from imp import reload\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from numpy import zeros\n",
    "from scipy.linalg import svd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk.corpus import words as nltk_words\n",
    "\n",
    "from scipy import misc\n",
    "from textblob import TextBlob\n",
    "import textacy\n",
    "import spacy\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "import gensim, logging\n",
    "from gensim import similarities\n",
    "from gensim.models import Phrases, word2vec, doc2vec\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import time\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "import seaborn as sns\n",
    "import string\n",
    "import math\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import linear_model\n",
    "\n",
    "from keras.layers import Activation\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "import vgg16; reload(vgg16)\n",
    "from vgg16 import Vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%cd ~/bdp_bipm/data\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Import Data From MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import scrapy\n",
    "\n",
    "class BlogSpider(scrapy.Spider):\n",
    "    name = 'blogspider'\n",
    "    start_urls = ['https://blog.scrapinghub.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for title in response.css('h2.entry-title'):\n",
    "            yield {'title': title.css('a ::text').extract_first()}\n",
    "\n",
    "        for next_page in response.css('div.prev-post > a'):\n",
    "            yield response.follow(next_page, self.parse)\n",
    "\n",
    "try:\n",
    "    # Python 3.x\n",
    "    from urllib.parse import quote_plus\n",
    "except ImportError:\n",
    "    # Python 2.x\n",
    "    from urllib import quote_plus\n",
    "#Connection to SERVER\n",
    "\n",
    "#Local connection\n",
    "uri = \"mongodb://localhost:27017\"\n",
    "\n",
    "client = MongoClient(uri)\n",
    "\n",
    "db = client.get_database(\"test\")\n",
    "assert db.name == 'test'\n",
    "\n",
    "class CNames:\n",
    "    id = \"_id\"\n",
    "    auto_category_id = \"auto_category_id\"\n",
    "    foreign_category_label = \"foreign_category_label\"\n",
    "    source = \"source\"\n",
    "    traderId = \"traderId\"\n",
    "    is_from_marketplace = \"is_from_marketplace\"\n",
    "    url = \"url\"\n",
    "    type = \"type\"\n",
    "    start_bid = \"start_bid\"\n",
    "    min_bid = \"min_bid\"\n",
    "    original_images = \"original_images\"\n",
    "    processed_images = \"processed_images\"\n",
    "    manufacturer_normalized = \"manufacturer_normalized\"\n",
    "    manufacturer = \"manufacturer\"\n",
    "    machine_type = \"machine_type\"\n",
    "    location = \"location\"\n",
    "    lang = \"lang\"\n",
    "    currency = \"currency\"\n",
    "    country = \"country\"\n",
    "    city = \"city\"\n",
    "    created_at = \"created_at\"\n",
    "    categoriesKeys = \"categoriesKeys\"\n",
    "    bid_count = \"bid_count\"\n",
    "    actual_bid = \"actual_bid\"\n",
    "    year_of_manufacture = \"year_of_manufacture\"\n",
    "    attributes = \"attributes\"\n",
    "    auctions = \"auctions\"\n",
    "    key = \"key\"\n",
    "    positionlat = \"position.lat\"\n",
    "    positionlon = \"position.lon\"\n",
    "    tags = \"tags\"\n",
    "    target = \"auto_category_id\"\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "experimentSize = 1000000\n",
    "\n",
    "\n",
    "\n",
    "lotsAttribListToFindInDb = {\"_id\":False,\"auto_category_id\":True,\"foreign_category_label\":True,\"source\":True,\"traderId\":True,\"is_from_marketplace\":True,\n",
    "                            \"url\":True,\"type\":True,\"translations\":True,\"start_bid\":True,\"min_bid\":True,\"original_images\":True,\"processed_images\":True,\"manufacturer_normalized\":True,\n",
    "                            \"manufacturer\":True,\"machine_type\":True,\"location\":True,\"lang\":True,\"currency\":True,\"country\":True,\"city\":True,\"created_at\":True,\n",
    "                            \"categoriesKeys\":True,\"bid_count\":True,\"actual_bid\":True,\"year_of_manufacture\":True,\"attributes\":True,\"position\":True}\n",
    "\n",
    "catsAttribListToFindInDb = {\"_id\":True,\"auctions\":True,\"created_at\":True,\"key\":True,\"translations\":True,\"position\":True,\"tags\":True,\"parent_id\":True}\n",
    "#sample_object = {'Name':'John', 'Location':{'City':'Los Angeles','State':'CA'}}\n",
    "#json_normalize(sample_object)\n",
    "\n",
    "\n",
    "lotsCursor = db.auctions.find({},lotsAttribListToFindInDb)\n",
    "catsCursor = db.catjsontest.find({},catsAttribListToFindInDb,limit = experimentSize)\n",
    "\n",
    "# Convert to DF\n",
    "\n",
    "#print(type(catsCursor[1:100]))\n",
    "#cats100_df = json_normalize(catsCursor[4])\n",
    "#replace print(i) with single json creation for DF\n",
    "def dfFromCursor(cur):\n",
    "    rawList = []\n",
    "    for i in cur:\n",
    "        rawList += [i]\n",
    "\n",
    "    return pd.DataFrame(json_normalize(rawList))\n",
    "\n",
    "# dtype={ColumnNames.actual_bid: np.uint32,\n",
    "#                                                        ColumnNames.bid_count: np.uint16,\n",
    "#                                                        ColumnNames.year_of_manufacture: np.uint16,\n",
    "#                                                        ColumnNames.start_bid: np.uint32,\n",
    "#                                                        ColumnNames.is_from_marketplace: np.bool_,\n",
    "#                                                        ColumnNames.positionlat: np.float32,\n",
    "#                                                        ColumnNames.positionlon: np.float32\n",
    "#                                                        }\n",
    "df_categories = dfFromCursor(catsCursor)\n",
    "df_lots = dfFromCursor(lotsCursor)\n",
    "\n",
    "df_lots.to_pickle(\"df_lots_all.pkl\")\n",
    "df_categories.to_pickle(\"df_categories.pkl\")\n",
    "#DONE\n",
    "df_lots = pd.read_pickle(\"df_lots1m.pkl\")\n",
    "df_categories = pd.read_pickle(\"df_categories.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pickle_dir = \"pickles/\"\n",
    "\n",
    "#stTagger = StanfordPOSTagger(model_filename='../stanford-postagger-full-2016-10-31/models/english-bidirectional-distsim.tagger', \n",
    "#                       path_to_jar= '../stanford-postagger-full-2016-10-31/stanford-postagger.jar')\n",
    "w2vModel = KeyedVectors.load_word2vec_format('glove_6B_50d.txt', binary=False)\n",
    "lancas = LancasterStemmer()\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "prepositions = set([\"by\",\"around\", \"at\", \"away\", \"down\", \"in\", \"off\", \"on\", \"out\", \"over\", \"round\", \"up\", \"upon\", \"onto\", \"through\", \"against\", \"under\", \"aside\", \"between\", \"before\", \"with\", \"without\", \"forward\", \"behind\", \"without\", \"across\", \"as\", \"back\", \"apart\", \"after\", \"away\", \"towards\", \"ahead\", \"along\", \"of\", \"past\", \"aback\", \"about\", \"above\", \"apart\", \"aside\", \"way\", \"to\"])\n",
    "negationWords = set([\"not\",\"without\",\"no\",\"never\"])\n",
    "POS_list = [\"$\",\"''\",\"(\",\")\",\",\",\"--\",\".\",\":\",\"CC\",\"CD\",\"DT\",\"EX\",\"FW\",\"IN\",\"JJ\",\"JJR\",\"JJS\",\"LS\",\"MD\",\"NN\",\"NNP\",\"NNPS\",\"NNS\",\"PDT\",\"POS\",\"PRP\",\"PRP$\",\"RB\",\"RBR\",\"RBS\",\"RP\",\"SYM\",\"TO\",\"UH\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\",\"WDT\",\"WP\",\"WP$\",\"WRB\",\"``\"]\n",
    "NE_list = set([\"ORGANIZATION\",\"PERSON\",\"LOCATION\",\"DATE\",\"TIME\",\"MONEY\",\"PERCENT\",\"FACILITY\",\"GPE\"])\n",
    "posToWn = {\"N\":wn.NOUN,\"V\":wn.VERB,\"J\":wn.ADJ,\"R\":wn.ADV}\n",
    "\n",
    "all_columns_set = set([\"actual_bid\",\n",
    "\"attributes\",\n",
    "\"auto_category_id\",\n",
    "\"bid_count\",\n",
    "\"categoriesKeys\",\n",
    "\"city\",\n",
    "\"country\",\n",
    "\"created_at\",\n",
    "\"currency\",\n",
    "\"foreign_category_label\",\n",
    "\"is_from_marketplace\",\n",
    "\"lang\",\n",
    "\"location\",\n",
    "\"machine_type\",\n",
    "\"manufacturer\",\n",
    "\"manufacturer_normalized\",\n",
    "\"min_bid\",\n",
    "\"original_images\",\n",
    "\"position\",\n",
    "\"position.lat\",\n",
    "\"position.lon\",\n",
    "\"processed_images\",\n",
    "\"source\",\n",
    "\"start_bid\",\n",
    "\"traderId\",\n",
    "\"translations.ar.description\",\n",
    "\"translations.ar.title\",\n",
    "\"translations.bg.description\",\n",
    "\"translations.bg.title\",\n",
    "\"translations.cs.description\",\n",
    "\"translations.cs.title\",\n",
    "\"translations.da.description\",\n",
    "\"translations.da.title\",\n",
    "\"translations.de.description\",\n",
    "\"translations.de.original_title\",\n",
    "\"translations.de.slug\",\n",
    "\"translations.de.title\",\n",
    "\"translations.en.description\",\n",
    "\"translations.en.original_title\",\n",
    "\"translations.en.slug\",\n",
    "\"translations.en.title\",\n",
    "\"translations.es.description\",\n",
    "\"translations.es.original_title\",\n",
    "\"translations.es.title\",\n",
    "\"translations.et.description\",\n",
    "\"translations.et.title\",\n",
    "\"translations.fi.description\",\n",
    "\"translations.fi.title\",\n",
    "\"translations.fr.description\",\n",
    "\"translations.fr.original_title\",\n",
    "\"translations.fr.title\",\n",
    "\"translations.hr.description\",\n",
    "\"translations.hr.title\",\n",
    "\"translations.hu.description\",\n",
    "\"translations.hu.title\",\n",
    "\"translations.it.description\",\n",
    "\"translations.it.title\",\n",
    "\"translations.ko.description\",\n",
    "\"translations.ko.title\",\n",
    "\"translations.nl.description\",\n",
    "\"translations.nl.title\",\n",
    "\"translations.no.description\",\n",
    "\"translations.no.title\",\n",
    "\"translations.pl.description\",\n",
    "\"translations.pl.title\",\n",
    "\"translations.ro.description\",\n",
    "\"translations.ro.title\",\n",
    "\"translations.ru.description\",\n",
    "\"translations.ru.title\",\n",
    "\"translations.sk.description\",\n",
    "\"translations.sk.title\",\n",
    "\"translations.sl.description\",\n",
    "\"translations.sl.title\",\n",
    "\"translations.sv.description\",\n",
    "\"translations.sv.title\",\n",
    "\"translations.tr.description\",\n",
    "\"translations.tr.title\",\n",
    "\"type\",\n",
    "\"url\"])\n",
    "\n",
    "numeric_columns_list = [\"year_of_manufacture\",\n",
    "\"bid_count\",\n",
    "\"start_bid\",\n",
    "\"actual_bid\",\n",
    "\"min_bid\",\n",
    "\"position.lat\",\n",
    "\"position.lon\",\n",
    "\"is_from_marketplace\"]\n",
    "\n",
    "translate_columns = [\n",
    "    \"translations.ar.description\",\n",
    "\"translations.ar.title\",\n",
    "\"translations.bg.description\",\n",
    "\"translations.bg.title\",\n",
    "\"translations.cs.description\",\n",
    "\"translations.cs.title\",\n",
    "\"translations.da.description\",\n",
    "\"translations.da.title\",\n",
    "\"translations.de.description\",\n",
    "\"translations.de.original_title\",\n",
    "\"translations.de.slug\",\n",
    "\"translations.de.title\",\n",
    "\"translations.en.description\",\n",
    "\"translations.en.original_title\",\n",
    "\"translations.en.slug\",\n",
    "\"translations.en.title\",\n",
    "\"translations.es.description\",\n",
    "\"translations.es.original_title\",\n",
    "\"translations.es.title\",\n",
    "\"translations.et.description\",\n",
    "\"translations.et.title\",\n",
    "\"translations.fi.description\",\n",
    "\"translations.fi.title\",\n",
    "\"translations.fr.description\",\n",
    "\"translations.fr.original_title\",\n",
    "\"translations.fr.title\",\n",
    "\"translations.hr.description\",\n",
    "\"translations.hr.title\",\n",
    "\"translations.hu.description\",\n",
    "\"translations.hu.title\",\n",
    "\"translations.it.description\",\n",
    "\"translations.it.title\",\n",
    "\"translations.ko.description\",\n",
    "\"translations.ko.title\",\n",
    "\"translations.nl.description\",\n",
    "\"translations.nl.title\",\n",
    "\"translations.no.description\",\n",
    "\"translations.no.title\",\n",
    "\"translations.pl.description\",\n",
    "\"translations.pl.title\",\n",
    "\"translations.ro.description\",\n",
    "\"translations.ro.title\",\n",
    "\"translations.ru.description\",\n",
    "\"translations.ru.title\",\n",
    "\"translations.sk.description\",\n",
    "\"translations.sk.title\",\n",
    "\"translations.sl.description\",\n",
    "\"translations.sl.title\",\n",
    "\"translations.sv.description\",\n",
    "\"translations.sv.title\",\n",
    "\"translations.tr.description\",\n",
    "\"translations.tr.title\"\n",
    "]\n",
    "\n",
    "analysed_languages = [\"de\",\"en\"]\n",
    "\n",
    "translation_attr = [\"description\",\"title\"]\n",
    "\n",
    "\n",
    "target_column = \"auto_category_id\"\n",
    "sample_of_rows_size = 100000 #\n",
    "sample_of_cats_size = 100 # \n",
    "#Init spacy nlp\n",
    "en_nlp = spacy.load('en')\n",
    "de_nlp = spacy.load('de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Tuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class TuningParameters:\n",
    "    title_importance = 1.0\n",
    "    form_phrases_count = 10\n",
    "    phrases_bigram_weight = 1.0\n",
    "    phrases_trigram_weight = 1.0\n",
    "    titles_weight = 1.0\n",
    "    fuzzy_word_threshold = 77\n",
    "    \n",
    "number_of_cores = max(multiprocessing.cpu_count() - 5,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Parallelism functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mydata = [{'subid' : 'B14-111', 'age': 75, 'fdg':1.78},\n",
    "          {'subid' : 'B14-112', 'age': 22, 'fdg':1.56},\n",
    "            {'subid' : 'B14-113', 'age': 24, 'fdg':1.59},\n",
    "         {'subid' : 'B14-111', 'age': 78, 'fdg':1.88},\n",
    "          {'subid' : 'B14-111', 'age': 32, 'fdg':1.06},\n",
    "            {'subid' : 'B14-113', 'age': 34, 'fdg':1.19},\n",
    "         {'subid' : 'B14-111', 'age': 75, 'fdg':1.78},\n",
    "          {'subid' : 'B14-112', 'age': 22, 'fdg':1.56},\n",
    "            {'subid' : 'B14-113', 'age': 24, 'fdg':1.59},\n",
    "         {'subid' : 'B14-111', 'age': 78, 'fdg':1.88},\n",
    "          {'subid' : 'B14-112', 'age': 32, 'fdg':1.06},\n",
    "            {'subid' : 'B14-113', 'age': 34, 'fdg':1.19}]\n",
    "df = pd.DataFrame(mydata)\n",
    "\n",
    "def _apply_df(args):\n",
    "    df, func, kwargs = args\n",
    "    return func(df,**kwargs) \n",
    "\n",
    "def apply_by_multiprocessing(df, func,  **kwargs):\n",
    "    workers = kwargs.pop('workers')\n",
    "    repeat = kwargs.pop('repeat')\n",
    "    \n",
    "    for i in range(repeat):\n",
    "        pool = multiprocessing.Pool(processes=workers)\n",
    "        result = pool.map(_apply_df, [(d, func, kwargs)\n",
    "            for d in np.array_split(df, workers<<1)])\n",
    "        pool.close()\n",
    "        #pool.join()\n",
    "        df = pd.concat(result)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def apply_by_multiprocessing_corpus(df, func,  **kwargs):\n",
    "    workers = kwargs.pop('workers')\n",
    "    \n",
    "    pool = multiprocessing.Pool(processes=workers)\n",
    "    n = workers<<1\n",
    "    result = pool.map(_apply_df, [(d, func, kwargs)\n",
    "        for d in [df[i:i + n] for i in range(0, len(df), n)]])\n",
    "    \n",
    "    pool.close()\n",
    "    return sum(result)\n",
    "\n",
    "\n",
    "def group_by_target_column(dff):\n",
    "    return dff.groupby(\"subid\").agg(sum)\n",
    "\n",
    "def sqrr(x):\n",
    "    return sum(x)\n",
    "\n",
    "print(group_by_target_column(apply_by_multiprocessing(df,group_by_target_column,repeat=2,workers = 3)))\n",
    "apply_by_multiprocessing_corpus([1,2,3,4,5,6,7,8,9,10],sqrr,workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Read input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_lots = pd.read_pickle(pickle_dir+\"df_lots_all.pkl\")\n",
    "df_categories = pd.read_pickle(pickle_dir+\"df_categories.pkl\")\n",
    "\n",
    "df_lots = df_lots[df_lots[target_column].notnull()]\n",
    "def boolToInt(x):\n",
    "    if x == \"True\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "df_lots[\"is_from_marketplace\"] = df_lots[\"is_from_marketplace\"].apply(boolToInt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Preprocessing Data for exploration and further use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Init. of list of columns for text mining "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Create dataframe by concatenated titles and descriptions grouped by categories\n",
    "grouping_non_translate_columns = [\n",
    "\"foreign_category_label\",\n",
    "\"machine_type\",\n",
    "\"manufacturer\",\n",
    "\"manufacturer_normalized\",target_column]+[\"translations.%s.%s\" % (lang, trans) for lang in analysed_languages for trans in translation_attr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Slice text mining columns from DF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grouping_titles_descriptions = df_lots[grouping_non_translate_columns].sample(sample_of_rows_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sample_df_lots = df_lots.sample(sample_of_rows_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sample_df_lots = sample_df_lots[grouping_non_translate_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Util. functions for grouping dataframe by target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def en_text_nlp(st):\n",
    "    #print(\"ser \",en_nlp(str(series)))\n",
    "    if type(st) == str:\n",
    "        #print(\"st = \" ,en_nlp(st.rstrip()))\n",
    "        return en_nlp(str(st))\n",
    "    else:\n",
    "        return en_nlp(\"\")\n",
    "    \n",
    "\n",
    "def de_text_nlp(st):\n",
    "    #print(\"ser \",en_nlp(str(series)))\n",
    "    if type(st) == str:\n",
    "        #print(\"st = \" ,en_nlp(st.rstrip()))\n",
    "        return de_nlp(str(st))\n",
    "    else:\n",
    "        return de_nlp(\"\")\n",
    "\n",
    "def series_nlp(series):\n",
    "    if series.name != target_column:\n",
    "        if \".de.\" not in series.name:\n",
    "            return series.where(series.isnull(),en_text_nlp)\n",
    "        else:\n",
    "            return series.where(series.isnull(),de_text_nlp)\n",
    "    else:\n",
    "        return series\n",
    "\n",
    "def text_sum(series):\n",
    "    if series[series.notnull()].size > 1:\n",
    "        return reduce(lambda x, y: str(x) +\" \" + str(y), series[series.notnull()])\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "def text_sum_nlp(series):\n",
    "    if series[series.notnull()].size > 1:\n",
    "        txt = reduce(lambda x, y: str(x) +\" \" + str(y), series[series.notnull()])\n",
    "        if \".de.\" not in series.name:\n",
    "            #print(txt)\n",
    "            return en_text_nlp(txt)\n",
    "        else:\n",
    "            return de_text_nlp(txt)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def df_text_grouped_nlp(dff,**kwargs):\n",
    "    return dff.groupby(target_column).agg(text_sum_nlp)\n",
    "\n",
    "\n",
    "def par_doc_series(series,lang_nlp):\n",
    "    return pd.Series(np.array([doc for doc in lang_nlp.pipe(series.values, batch_size=400,n_threads=number_of_cores)]))\n",
    "        \n",
    "def df_text_nlp(dff):\n",
    "    #dff.drop(target_column,axis=1)[[\"translations.en.title\",\"translations.en.description\"]]\n",
    "    d = {\n",
    "     \"translations.en.title\":par_doc_series(dff[\"translations.en.title\"].astype(str),en_nlp),\n",
    "     \"translations.en.description\":par_doc_series(dff[\"translations.en.description\"].astype(str),en_nlp),\n",
    "     \"translations.de.title\":par_doc_series(dff[\"translations.de.title\"].astype(str),de_nlp),\n",
    "     \"translations.de.description\":par_doc_series(dff[\"translations.de.description\"].astype(str),de_nlp)\n",
    "    }\n",
    "    resDf = dff.loc[:,[target_column,\"foreign_category_label\",\"machine_type\",\"manufacturer\",\"manufacturer_normalized\"]]\n",
    "    for k,v in d.items():\n",
    "        resDf[k] = v\n",
    "    \n",
    "    return resDf\n",
    "\n",
    "def group_by_target_column_agg(dff):\n",
    "    return df_text_nlp(dff.groupby(target_column).agg(text_sum).copy())\n",
    "\n",
    "def isBothGtThan0(st1,st2):\n",
    "    return len(st1)>0 and len(st2)>0\n",
    "\n",
    "def fuzzyMatch(st1,st2):\n",
    "    \"\"\"\n",
    "    Just fuzzy partial_token_sort_ratio(https://pypi.python.org/pypi/fuzzywuzzy), which returns the distance between two words\n",
    "    \"\"\"\n",
    "    if not isBothGtThan0(st1,st2):\n",
    "        return None\n",
    "    \n",
    "    return fuzz.partial_token_sort_ratio(st1,st2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Failed experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# starttime = time.time()   \n",
    "# if __name__ == '__main__':\n",
    "#     #grouped_titles_descriptions = apply_by_multiprocessing(grouping_titles_descriptions[:10000], group_by_target_column_agg, repeat=1, axis=1,workers = number_of_cores)\n",
    "#     grouped_titles_descriptions = group_by_target_column_agg(grouping_titles_descriptions[:100000])\n",
    "#     print(\"Grouped done!\")\n",
    "    \n",
    "#     #ungrouped_titles_descriptions = df_text_nlp(grouping_titles_descriptions[:100])\n",
    "    \n",
    "#     ungrouped_titles_descriptions = df_text_nlp(grouping_titles_descriptions[:100000])\n",
    "#     print(\"Ungrouped done!\")\n",
    "# print(\"Operation completed in \",time.time()-starttime,\" seconds\")\n",
    "#Full require Operation completed in  11929.308161735535  seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#starttime = time.time()    \n",
    "#print(\"Pickling started!\")\n",
    "#grouped_titles_descriptions.to_pickle(pickle_dir+\"grouped_by_cat_text_columns.pkl\")\n",
    "#ungrouped_titles_descriptions.to_pickle(pickle_dir+\"ungrouped_by_cat_text_columns.pkl\")\n",
    "#grouped_titles_descriptions.info()\n",
    "#print(\"Operation completed in \",time.time()-starttime,\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#spacy doc and pickle is uncompactable \n",
    "#grouped_titles_descriptions = pd.read_pickle(pickle_dir+\"grouped_by_cat_text_columns_all.pkl\")\n",
    "#ungrouped_titles_descriptions = pd.read_pickle(pickle_dir+\"ungrouped_by_cat_text_columns_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#print(grouped_titles_descriptions.shape)\n",
    "#print(ungrouped_titles_descriptions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Corpuses and DocTermMatrixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Group DF by category by text mining columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#grouped_titles_descriptions = grouping_titles_descriptions.groupby(target_column).agg(text_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#grouped_titles_descriptions.to_pickle(pickle_dir+\"grouped_by_cat_text_columns.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#grouped_titles_descriptions.loc[:,[\"translations.%s.%s\" % (lang, \"description\") for lang in analysed_languages]] = [[\"12\",\"13\"]]\n",
    "#grouped_titles_descriptions.loc[\"531600ae78f860d0398b4567\",\"translations.en.description\"]=[\"dasfaf\",\"fsaf f@%%\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "set_of_cols = list(set(grouping_non_translate_columns).difference(set([target_column])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grouped_titles_descriptions= pd.read_pickle(pickle_dir+\"grouped_by_cat_text_columns.pkl\")\n",
    "\n",
    "def flatten1Level(ser):\n",
    "    norm_grouped_titles_descriptions = []\n",
    "    for i in ser.values.tolist():\n",
    "        if type(i) == list:\n",
    "            if len(i)>1:\n",
    "                j = reduce(lambda x, y: x +\" \" + y, i)                \n",
    "                norm_grouped_titles_descriptions += [j]\n",
    "            else:\n",
    "                norm_grouped_titles_descriptions += i\n",
    "        else:\n",
    "            norm_grouped_titles_descriptions += [i]\n",
    "        \n",
    "    return pd.Series(np.array(norm_grouped_titles_descriptions),index = ser.index)\n",
    "#Flatten descriptions\n",
    "\n",
    "\n",
    "Combine all columns to get \"full\" description\n",
    "grouped_titles_descriptions[[\"translations.%s.%s\" % (lang, \"description\") for lang in analysed_languages]] = grouped_titles_descriptions[[\"translations.%s.%s\" % (lang, \"description\") for lang in analysed_languages]].apply(flatten1Level, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "####  Rename indexes of Series (draft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#for lang in analysed_languages:\n",
    "#    grouped_titles_descriptions[\"translations.%s.description\" % lang] = test[\"translations.%s.description\" % lang].rename({i:grouped_titles_descriptions[\"translations.%s.description\" % lang].index[i] for i in range(869)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "set_of_cols_en = list(set(set_of_cols).difference(set([\"translations.de.title\",\"translations.de.description\"])))\n",
    "set_of_cols_de = list(set(set_of_cols).difference(set([\"translations.en.title\",\"translations.en.description\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def convertToEnDocSeries(x):\n",
    "    return x.apply(textacy.Doc,args=(None,\"en\"))\n",
    "\n",
    "def convertToEnDoc(x):\n",
    "    return textacy.Doc(x,lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sample_df_lots[\"transtions.en.full.description\"] = sample_df_lots.loc[:,set_of_cols_en].apply(text_sum,reduce = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sample_df_lots.set_index(target_column,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sample_df_lots[\"transtions.en.full.description\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sample_df_lots[\"transtions.en.full.description_doc\"] = sample_df_lots.loc[:,\"transtions.en.full.description\"].apply(convertToEnDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grouped_titles_descriptions[\"transtions.en.full.description\"] = grouped_titles_descriptions[set_of_cols_en].apply(text_sum,reduce = True, axis = 1)\n",
    "grouped_titles_descriptions[\"transtions.de.full.description\"] = grouped_titles_descriptions[set_of_cols_de].apply(text_sum,reduce = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#grouped_titles_descriptions.to_pickle(pickle_dir+\"grouped_by_cat_text_columns_full.pkl\")\n",
    "grouped_titles_descriptions = pd.read_pickle(pickle_dir+\"grouped_by_cat_text_columns_full.pkl\").sample(sample_of_cats_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grouped_titles_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "_DEFAULT_N_THREADS = max(multiprocessing.cpu_count() - 1, 1)\n",
    "class ParCorpus(textacy.Corpus):\n",
    "    def add_texts(self, texts, metadatas=None,\n",
    "                  n_threads=_DEFAULT_N_THREADS, batch_size=1000):\n",
    "        super(ParCorpus, self).add_texts(texts, metadatas=metadatas,n_threads=n_threads, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# buf = open(\"error.json\",\"w\")\n",
    "# test_str = records_en[5]['value'].text\n",
    "# buf.write(test_str)\n",
    "# buf.close()#21904"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Generate text stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def seriesToDict(ser):\n",
    "    return [{\"key\":key,\"value\":value} for key, value in zip(ser.index,ser)]\n",
    "\n",
    "def streamAndMeta(ser):\n",
    "    return textacy.fileio.split_record_fields(seriesToDict(ser), 'value')\n",
    "\n",
    "def corpusFromSeriesDoc(ser,lang=\"en\"):\n",
    "    text_stream_en, metadata_stream_en = streamAndMeta(ser)\n",
    "    return textacy.Corpus(lang, docs=text_stream_en, metadatas = metadata_stream_en)\n",
    "\n",
    "def corpusFromSeriesText(ser,lang=\"en\"):\n",
    "    text_stream_en, metadata_stream_en = streamAndMeta(ser)\n",
    "    return textacy.Corpus(lang, texts=text_stream_en, metadatas = metadata_stream_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "starttime = time.time()\n",
    "# Check if the word exist \"word\" in words.words()\n",
    "#open file example\n",
    "#for subdir, dirs, files in os.walk(vendors_dir):\n",
    "#    for file in files:\n",
    "#        f = open(os.path.join(subdir, file), errors='ignore')\n",
    "#        erpDocs[file] = textacy.preprocess.fix_bad_unicode(textacy.preprocess.normalize_whitespace(f.read()))\n",
    "#records_en = [{\"key\":key,\"value\":\"# \"+value.encode('utf8').decode('utf8','replace')} for key, value in grouped_titles_descriptions[\"transtions.en.full.description\"].to_dict().items()]\n",
    "records_en = [{\"key\":key,\"value\":\"# \"+value} for key, value in grouped_titles_descriptions[\"transtions.en.full.description\"].to_dict().items()]\n",
    "records_de = [{\"key\":key,\"value\":\"# \"+value} for key, value in grouped_titles_descriptions[\"transtions.de.full.description\"].to_dict().items()]\n",
    "\n",
    "print(\"Finish time \",time.time()-starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "phrases_en = Phrases([f['value'].split(\" \") for f in records_en], min_count=TuningParameters.form_phrases_count, threshold=7)\n",
    "\n",
    "bigramPhraserEn = Phraser(phrases_en)\n",
    "\n",
    "phrases_de = Phrases([f['value'].split(\" \") for f in records_de], min_count=TuningParameters.form_phrases_count, threshold=7)\n",
    "\n",
    "bigramPhraserDe = Phraser(phrases_de)\n",
    "\n",
    "# sentence_stream_phrases = bigramPhraserEn[sentence_stream]\n",
    "\n",
    "# trigram = Phrases(bigramPhraserEn[sentence_stream])\n",
    "\n",
    "# sent = [u'the', u'new', u'york', u'times', u'is', u'a', u'newspaper']\n",
    "# print(trigram[bigram[sent]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "records_de_c = copy.deepcopy(records_de)\n",
    "records_en_c = copy.deepcopy(records_en)\n",
    "\n",
    "for f in records_en_c:\n",
    "    f['value'] = \" \".join(bigramPhraserEn[f['value'].split(\" \")])\n",
    "\n",
    "for f in records_de_c:\n",
    "    f['value'] = \" \".join(bigramPhraserDe[f['value'].split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "starttime = time.time()\n",
    "text_stream_de, metadata_stream_de = textacy.fileio.split_record_fields(records_de, 'value')\n",
    "text_stream_en, metadata_stream_en = textacy.fileio.split_record_fields(records_en, 'value')\n",
    "corpuses = {}\n",
    "corpuses['normal.en'] = ParCorpus(\"en\", texts=text_stream_en, metadatas = metadata_stream_en)\n",
    "corpuses['normal.de'] = ParCorpus('de', texts=text_stream_de, metadatas = metadata_stream_de)\n",
    "\n",
    "text_stream_de, metadata_stream_de = textacy.fileio.split_record_fields(records_de_c, 'value')\n",
    "text_stream_en, metadata_stream_en = textacy.fileio.split_record_fields(records_en_c, 'value')\n",
    "corpuses['phrases.en'] = ParCorpus(\"en\", texts=text_stream_en, metadatas = metadata_stream_en)\n",
    "corpuses['phrases.de'] = ParCorpus('de', texts=text_stream_de, metadatas = metadata_stream_de)\n",
    "print(\"Finish time \",time.time()-starttime)\n",
    "#for 35 Finish time  1040.6533074378967"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lot_corpuses = {}\n",
    "lot_corpuses['normal.en.10k'] = corpusFromSeriesText(sample_df_lots[\"transtions.en.full.description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lot_corpuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lot_corpuses['normal.en'][1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sample_df_lots.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Noun chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "def plusWithSpace(x,y):\n",
    "    return x+\" \"+y\n",
    "\n",
    "def reduceWithUnderScore(l):\n",
    "    reduce(plusWithSpace,l)\n",
    "res = []    \n",
    "\n",
    "def callback(result):\n",
    "    res.append(result)\n",
    "    \n",
    "\n",
    "def listoflistTOlistofstrLabel(lol):\n",
    "    pool = multiprocessing.Pool(processes=number_of_cores)\n",
    "    \n",
    "    for doc in lol:\n",
    "        pool.apply_async(reduceWithUnderScore, args = ([token.text.replace(\" \",\"_\") for token in doc],),callback=callback)\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return res\n",
    "    #return [reduce(lambda x,y:x+\" \"+y,[token.text.replace(\" \",\"_\") for token in doc]) for doc in lol]\n",
    "\n",
    "sentence_stream_noun = listoflistTOlistofstrLabel([[w for w in textacy.extract.noun_chunks(doc)] for doc in corpuses['normal.en'][:2]])\n",
    "#pattern = textacy.constants.POS_REGEX_PATTERNS['en']['NP']\n",
    "print(\"Finish time \",time.time()-starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Test of chunk\n",
    "#len(list(textacy.extract.pos_regex_matches(doc, pattern)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "corpuses[\"chunk.en\"] = textacy.Corpus('en', texts=sentence_stream_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#TOO Time consuming\n",
    "#corpuses[\"chunk.de\"] = textacy.Corpus('de', texts=sentence_stream_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "corpuses['normal.de'].save(pickle_dir+\"docs\", name='Normal_Grouped_Desc_Title_DE_100', compression='gzip')\n",
    "corpuses['normal.en'].save(pickle_dir+\"docs\", name='Normal_Grouped_Desc_Title_EN_100', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# corpuses['normal.de'].save(pickle_dir+\"docs\", name='Normal_Grouped_Desc_Title_DE', compression='gzip')\n",
    "# corpuses['normal.en'].save(pickle_dir+\"docs\", name='Normal_Grouped_Desc_Title_EN', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "corpuses['normal.de'] = textacy.Corpus.load(pickle_dir+\"docs\", name='Normal_Grouped_Desc_Title_DE', compression='gzip')\n",
    "corpuses['normal.en'] = textacy.Corpus.load(pickle_dir+\"docs\", name=\"Normal_Grouped_Desc_Title_EN\", compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "starttime = time.time()\n",
    "corpuses['normal.de'] = textacy.Corpus.load(pickle_dir+\"docs\", name='Normal_Grouped_Desc_Title_DE_35', compression='gzip')\n",
    "corpuses['normal.en'] = textacy.Corpus.load(pickle_dir+\"docs\", name=\"Normal_Grouped_Desc_Title_EN_35\", compression='gzip')\n",
    "print(\"Success! in \",time.time()-starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Alternative pickling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i,doc in enumerate(corpuses['normal.en']):\n",
    "\n",
    "#     try:\n",
    "#         doc.save(pickle_dir+\"docs/en\",name=\"doc\"+str(i))\n",
    "#     except KeyError:\n",
    "        \n",
    "#         print(doc,\" number \"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# for i,doc in enumerate(corpuses['normal.en']):\n",
    "#     #corpuses['normal.de'].save(pickle_dir+\"docs\", name='Normal_Grouped_Desc_Title_DE', compression='gzip')\n",
    "#     #corpuses['normal.en'].save(pickle_dir+\"docs\", name='Normal_Grouped_Desc_Title_EN')\n",
    "#     try:\n",
    "#         doc.save(pickle_dir+\"docs/de\",name=\"doc\"+str(i))\n",
    "#     except KeyError:\n",
    "#         print(doc,\" number \"+str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Error line checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# exDoc = en_nlp(corpuses['normal.en'][31].text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "file = open(pickle_dir+\"testfile2.txt\",\"w\")\n",
    "file.write(textacy.preprocess.fix_bad_unicode(corpuses['normal.en'][31].text))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#[[token.orth_ for token in textacy.extract.words(doc, filter_stops=True, filter_punct=True, filter_nums=True)] for doc in corpuses['normal'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Not words list just for English, for German doesn't make a sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nltk_dictionary = dict.fromkeys(nltk_words.words(), None)\n",
    "\n",
    "def is_english_word(word):\n",
    "    try:\n",
    "        x = dictionary[word]\n",
    "        return True\n",
    "    except KeyError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Add category names to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Data exploration\n",
    "df_lots.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "corpuses['normal.de']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpuses['normal.en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def createStats(corp):\n",
    "    stats_array = []\n",
    "    for doc in corp:\n",
    "        if len(doc)>1:\n",
    "            ts = textacy.text_stats.TextStats(doc)\n",
    "            d = ts.basic_counts\n",
    "            d[\"readability\"] = ts.flesch_kincaid_grade_level\n",
    "            stats_array += [d]\n",
    "    return stats_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stats_list = createStats(corpuses['normal.en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame(stats_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stats_df = pd.read_csv(\"~/stats_for_100_random_categories.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stats_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#pipeline = Pipeline([\n",
    "#    ('vect', CountVectorizer()),\n",
    "#    ('tfidf', TfidfTransformer()),\n",
    "#    ('clf', SGDClassifier()),\n",
    "#])\n",
    "stats_df = pd.DataFrame(scaler.fit_transform(stats_df[stats_df<400000]), columns=stats_df.columns)\n",
    "pltt = sns.boxplot(stats_df)\n",
    "plt.xticks(rotation=45)\n",
    "#pltt.save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nullsSeries = np.sum(df_lots.isnull())\n",
    "#Nulls distribution\n",
    "sns.boxplot(nullsSeries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nonNumericNonTranslationColumns = all_columns_set.difference(set(numeric_columns_list)).difference(set(translate_columns))\n",
    "# f, subplotax = plt.subplots(4,5)\n",
    "# f.tight_layout()\n",
    "# f\n",
    "\n",
    "ind = np.arange(20)\n",
    "width = 0.35\n",
    "fig = plt.figure()\n",
    "plt.bar(ind, [np.sum(df_lots[col].isnull()) for col in nonNumericNonTranslationColumns], width, color='r')\n",
    "plt.xticks(ind, nonNumericNonTranslationColumns, rotation='vertical')\n",
    "fig.savefig(\"nonTranslationNulls.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_lots.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_lots.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Explore format of attributes\n",
    "df_lots[\"attributes\"].iloc[999970][0][\"language\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#How many images do we have\n",
    "all_images_by_category = pd.read_pickle(pickle_dir+\"all_images_by_category.pkl\")\n",
    "\n",
    "#image_urls = all_images_list[:sample_sample_size]\n",
    "\n",
    "#df_lots[df_lots[\"foreign_category_label\"].notnull()].apply( )\n",
    "#Plot title and desc word distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_images_by_category[\"amount\"] = all_images_by_category[\"processed_images\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig2 = sns.boxplot(\"amount\",data=all_images_by_category[all_images_by_category[\"amount\"]<500])\n",
    "fig2.get_figure().savefig(\"numberofimagespercategory.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_images_by_category.to_csv(\"~/images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "en_desc_length = df_lots[df_lots[\"translations.en.description\"].notnull()].loc[:10000,\"translations.en.description\"].apply(lambda x:len(en_nlp(x)))\n",
    "en_title_length = df_lots[df_lots[\"translations.en.title\"].notnull()].loc[:10000,\"translations.en.title\"].apply(lambda x:len(en_nlp(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "en_desc_length = sample_df_lots[\"translations.en.title\"].apply(lambda x:len(str(x).split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = stats_df[stats_df[\"n_words\"]<320000]\n",
    "#sns.distplot(d[\"n_words\"])#.save\n",
    "d[\"n_words\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(en_desc_length[en_desc_length<100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(stats_df[\"n_words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(en_desc_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# length_prep_td = ungrouped_titles_descriptions.copy()\n",
    "# length_prep_td[\"category\"] = length_prep_td.index\n",
    "# length_prep_td[\"length\"] = ungrouped_titles_descriptions[\"translations.en.description\"][ungrouped_titles_descriptions[\"translations.en.description\"].notnull()].apply(lambda x:len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# sns.pointplot(x=\"category\", y=\"length\",data=length_prep_td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ungrouped_titles_descriptions[\"translations.en.description\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Number of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(df_lots[\"auto_category_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Wordcloud\n",
    "title10000 = df_lots[df_lots[\"translations.en.title\"].notnull()].loc[:100,\"translations.en.title\"].apply(lambda x:en_nlp(x).text).agg(\"sum\" )\n",
    "wordcloud = WordCloud().generate(title10000)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "type(title10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "finalDF = df_lots[numeric_columns_list].sample(sample_of_rows_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "finalDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Simple features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lots_sample_df = df_lots.loc[finalDF.index.values.tolist(),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Foreign category fuzzy match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_categories[[\"translations.en.title\",\"translations.de.title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "finalSDF = pd.DataFrame(pd.Series(np.zeros(sample_of_rows_size),index = finalDF.index.values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fuzzMatch(st1,st21,st22):\n",
    "    try:\n",
    "        res = max((fuzz.ratio(st1,st21),fuzz.ratio(st1,st22)))\n",
    "    except:\n",
    "        res = 0\n",
    "    if res > TuningParameters.fuzzy_word_threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "gen = df_categories[[\"translations.en.title\",\"translations.de.title\"]].iterrows()\n",
    "for row in gen:\n",
    "    finalSDF[\"foreign_cat_match\"+str(row[0])] = lots_sample_df[\"foreign_category_label\"].apply(fuzzMatch,args=(row[1][0],row[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "finalSDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Foreign category in \"full\" description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Advanced features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w2vDocmodel = {}\n",
    "for k,corpus in corpuses:\n",
    "    w2vDocmodel[k] += [word2vec.Word2Vec(doc.tokenized_text, size=64, window=5, min_count=1, workers=10, hs=1, negative=0)  for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.avg([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#corpuses['normal.en'][1][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for w2vectorDocs in w2vDocmodel[\"normal.en\"]:\n",
    "    for i,w2vectorDoc in enumerate(w2vectorDocs):\n",
    "        finalSDF[\"w2v\"+str(i)] = sample_df_lots[\"translations.en.full.description\"].apply(lambda x:w2vectorDoc.score([x.split(\" \")])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "w2vPharsermodel = word2vec.Word2Vec(bigramPhraser[corpuses['normal.en'].text], size = 50)\n",
    "\n",
    "# for i,w2v in enumerate(w2vDocmodel):\n",
    "#     print(\"Document \"+ str(i))\n",
    "#     for crit in criteria_term_list:\n",
    "#         print(\"Matrix = \",w2v.score(crit))\n",
    "#         print(\"Sum = \", sum(w2v.score(crit)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# LSA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Vectorize TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "docTermMatrix10 = []\n",
    "id2term = []\n",
    "vectorizers = []\n",
    "#docCriteriaTermIds = []\n",
    "for cor in lot_corpuses.values():\n",
    "    term_list = (doc.to_terms_list(ngram=1, named_entities = False, as_strings = True, filter_stops=True) for doc in cor)\n",
    "    y = [doc.metadata[\"key\"] for doc in cor]\n",
    "    vectorizer = textacy.vsm.Vectorizer(weighting='tfidf', normalize=True, smooth_idf=True,min_df=2, max_df=0.95)\n",
    "    docTermMatrix10 += [vectorizer.fit_transform(term_list)]\n",
    "    vectorizers += [vectorizer]\n",
    "    id2term += [vectorizer.id_to_term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "docTermMatrix10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "docTermMatrix = []\n",
    "id2term = []\n",
    "vectorizers = []\n",
    "#docCriteriaTermIds = []\n",
    "for cor in corpuses.values():\n",
    "    term_list = (doc.to_terms_list(ngram=1, named_entities = False, as_strings = True, filter_stops=True) for doc in cor)\n",
    "    vectorizer = textacy.vsm.Vectorizer(weighting='tfidf', normalize=True, smooth_idf=True,min_df=2, max_df=0.95)\n",
    "    docTermMatrix += [vectorizer.fit_transform(term_list)]\n",
    "    vectorizers += [vectorizer]\n",
    "    id2term += [vectorizer.id_to_term]\n",
    "    \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import sparse_random_matrix\n",
    "X = sparse_random_matrix(100, 100, density=0.01, random_state=42)\n",
    "svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "svd.fit(X) \n",
    "TruncatedSVD(algorithm='randomized', n_components=5, n_iter=7,\n",
    "        random_state=42, tol=0.0)\n",
    "print(svd.explained_variance_ratio_) \n",
    "[ 0.0606... 0.0584... 0.0497... 0.0434... 0.0372...]\n",
    "print(svd.explained_variance_ratio_.sum()) \n",
    "0.249...\n",
    "\n",
    "for ind,dtm in enumerate(docTermMatrix):\n",
    "    df[].apply()\n",
    "    term_list = (doc.to_terms_list(ngram=1, named_entities = False, as_strings = True, filter_stops=True) for doc in cor)\n",
    "    rowDTM = vectorizer[ind].fit_transform(term_list)\n",
    "    dotprod = np.dot(dtm,rowDTM.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "docTermMatrix = []\n",
    "id2term = []\n",
    "vectorizers = []\n",
    "#docCriteriaTermIds = []\n",
    "for cor in corpuses.values():\n",
    "    #self, ngrams=(1, 2, 3), named_entities=True,normalize='lemma', lemmatize=None, lowercase=None,as_strings=False\n",
    "    term_list = (doc.to_terms_list(ngram=1, named_entities = False, as_strings = True) for doc in cor)\n",
    "    vectorizer = textacy.vsm.Vectorizer(weighting='tf',normalize='lemma', min_df=2, max_df=0.95)\n",
    "    docTermMatrix += [vectorizer.fit_transform(term_list)]\n",
    "    vectorizers += [vectorizer]\n",
    "    id2term += [vectorizer.id_to_term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "m = docTermMatrix[1]\n",
    "#sdf = pd.SparseDataFrame([ pd.SparseSeries(m[i].toarray().ravel()) for i in np.arange(m.shape[0]) ]).to_csv(\"~/DTM_100_lemma_ngram1_ne0.csv\")\n",
    "\n",
    "np.savetxt(\"~/DTM_100_lemma_ngram1_ne0_de.csv\", m.todense(), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "m = docTermMatrix[0]\n",
    "np.savetxt(\"~/DTM_100_lemma_ngram1_ne0_en_50k.csv\", m[:,:50000].todense(), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Topic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# docTopicMatrix = []\n",
    "# docTopicModels = []\n",
    "\n",
    "# for dtm in docTermMatrix:\n",
    "#     model = textacy.tm.TopicModel(\"nmf\",n_topics = 5)\n",
    "#     model.fit(dtm)\n",
    "#     docTopicModels += [model]\n",
    "#     docTopicMatrix += [model.transform(dtm)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i,model in enumerate(docTopicModels):\n",
    "    #model.tra\n",
    "    #model.termite_plot(criteriaCorpDTM,criteriaId2term\n",
    "    if (type(criteriaCorpDTM[i])== list and len(criteriaCorpDTM[i])==0)or(hasattr(criteriaCorpDTM,\"shape\") and criteriaCorpDTM[i].shape[0]==0):\n",
    "        print('Cannot generate'+str(i))\n",
    "        continue\n",
    "    dotprod = np.dot(model.transform(criteriaCorpDTM[i]),docTopicMatrix[i].transpose())\n",
    "    print(dotprod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Resize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "path = \"~/bdp_bipm/data/images/\"\n",
    "\n",
    "def saveIMG(subdir,file,path):\n",
    "    im = Image.open(path)\n",
    "    f, e = os.path.splitext(path)\n",
    "    imResize = im.resize((224,224), Image.ANTIALIAS)\n",
    "    fname= subdir.replace(\"/bdp_bipm/data/images/\", \"/bdp_bipm/data/images_224/\")+\"/\"+ file.replace(\".jpg\",\"\") + '_resized.jpg'\n",
    "    os.makedirs(os.path.dirname(fname), exist_ok=True)\n",
    "    #print(fname)\n",
    "    imResize.save(fname, 'JPEG', quality=90)\n",
    "    \n",
    "def resize(p):\n",
    "    pool = multiprocessing.Pool(processes=number_of_cores)\n",
    "    for subdir, dirs, files in os.walk(p):\n",
    "        for file in files:\n",
    "            path = os.path.join(subdir, file)\n",
    "            if os.path.isfile(path):\n",
    "                pool.apply_async(saveIMG, args = (subdir,file,path))\n",
    "                \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "                \n",
    "resize(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "images_path = \"~/bdp_bipm/data/images_224/\"\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Divide images into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "image_validation_set_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_root = \"~/bdp_bipm/data/images_224/train/\"\n",
    "valid_root = \"~/bdp_bipm/data/images_224/valid/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# for item in os.listdir(train_root):\n",
    "#     num_of_images = len(os.listdir(train_root+item))\n",
    "#     proc_num = np.min([image_validation_set_size,int(num_of_images*0.15)])\n",
    "#     if proc_num == 0:\n",
    "#         continue\n",
    "#     else:\n",
    "#         os.makedirs(valid_root+item, exist_ok=True)\n",
    "#         for i in range(proc_num):\n",
    "#             newFname = random.choice(os.listdir(train_root+item))\n",
    "#             src = train_root+item+\"/\"+ newFname\n",
    "#             dest = valid_root+item+\"/\"+ newFname\n",
    "#             shutil.copyfile(src,dest)\n",
    "#             os.rename(src,train_root+item+\"/clr\"+ newFname)\n",
    "#     #print(random.choice(os.listdir(train_root+item)))\n",
    "#     #print(len(os.listdir(train_root+item)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import utils; reload(utils)\n",
    "# from utils import plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vgg = Vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Grab a few images at a time for training and validation.\n",
    "#NB: They must be in subdirectories named based on their category\n",
    "batches = vgg.get_batches(images_path+'train')\n",
    "val_batches = vgg.get_batches(images_path+'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vgg.finetune(batches)\n",
    "vgg.fit(batches, val_batches, nb_steps=1000, nb_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vgg.finetune(batches)\n",
    "vgg.fit(batches, nb_steps=5000, nb_epoch=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "imgs,labels = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plots(imgs[:3], titles=labels[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vgg.predict(imgs[:3], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batches.num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vgg.finetune(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vgg.fit(batches, val_batches, nb_steps=3000, nb_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "imgs,labels = next(val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_array = vgg.test(images_path+'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(converted_features, converted_target, test_size = 0.2, random_state=21)\n",
    "\n",
    "imp = Imputer(missing_values=np.NaN, strategy='median', axis=0)\n",
    "converted_df = pd.DataFrame(imp.fit_transform(converted_df), columns=converted_df.columns)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#pipeline = Pipeline([\n",
    "#    ('vect', CountVectorizer()),\n",
    "#    ('tfidf', TfidfTransformer()),\n",
    "#    ('clf', SGDClassifier()),\n",
    "#])\n",
    "converted_df = pd.DataFrame(scaler.fit_transform(converted_df), columns=converted_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Naive Bayes and foundation for CrossValidation\n",
    "imputer = preprocessing.Imputer(np.nan)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "clf = GaussianNB()\n",
    "\n",
    "pipeline = Pipeline([(\"impute\",imputer),('scale', scaler), ('nb', clf)])\n",
    "\n",
    "def nonNumberToNan(x):\n",
    "    try:\n",
    "        int(x)\n",
    "        return x\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "X = df_lots[numeric_columns_list].apply(lambda row:row.apply(nonNumberToNan)).values\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(df_lots[target_column].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(docTermMatrix, y, test_size=0.23, random_state=42)\n",
    "#pipeline.fit(X_train, y_train)\n",
    "#pipeline.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train = docTermMatrix[:80000]\n",
    "y_train = y[:80000]\n",
    "X_valid =  docTermMatrix[80000:100000]\n",
    "y_valid = y[80000:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Random Forest H2O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import h2o\n",
    "\n",
    "h2o.init(max_mem_size = \"2G\")             #specify max number of bytes. uses all cores by default.\n",
    "h2o.remove_all()\n",
    "\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "\n",
    "covtype_df = h2o.import_file(os.path.realpath(\"../data/covtype.full.csv\"))\n",
    "\n",
    "\n",
    "# We import the full covertype dataset (581k rows, 13 columns, 10 numerical, 3 categorical) and then split the data 3 ways:  \n",
    "#   \n",
    "# 60% for training  \n",
    "# 20% for validation (hyper parameter tuning)  \n",
    "# 20% for final testing  \n",
    "# \n",
    "#  We will train a data set on one set and use the others to test the validity of the model by ensuring that it can predict accurately on data the model has not been shown.  \n",
    "#  \n",
    "#  The second set will be used for validation most of the time.  \n",
    "#  \n",
    "#  The third set will be withheld until the end, to ensure that our validation accuracy is consistent with data we have never seen during the iterative process. \n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "#split the data as described above\n",
    "train, valid, test = covtype_df.split_frame([0.6, 0.2], seed=1234)\n",
    "\n",
    "#Prepare predictors and response columns\n",
    "covtype_X = covtype_df.col_names[:-1]     #last column is Cover_Type, our desired response variable \n",
    "covtype_y = covtype_df.col_names[-1]    \n",
    "\n",
    "\n",
    "\n",
    "rf_v1 = H2ORandomForestEstimator(\n",
    "    model_id=\"rf_covType_v1\",\n",
    "    ntrees=200,\n",
    "    stopping_rounds=2,\n",
    "    score_each_iteration=True,\n",
    "    seed=1000000)\n",
    "\n",
    "\n",
    "\n",
    "rf_v1.train(covtype_X, covtype_y, training_frame=train, validation_frame=valid)\n",
    "\n",
    "\n",
    "rf_v1.score_history()\n",
    "\n",
    "\n",
    "# Here we can see the hit ratio table.\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "rf_v1.hit_ratio_table(valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=20)\n",
    "clf.fit(docTermMatrix[0], y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "param_grid = {\"max_depth\": [3, 7, None],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"max_features\": [3,6,9],\n",
    "              \"min_samples_split\": [2,6,8],\n",
    "              \"min_samples_leaf\": [2,6,8]}\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid, cv = 5)\n",
    "\n",
    "grid_search.fit(docTermMatrix, y_train)\n",
    "\n",
    "rf_pred = grid_search.predict_proba(X_test) #needed for the ROC curve\n",
    "rf_pred2 = grid_search.predict(X_test) #needed for the confusion matrix\n",
    "\n",
    "print(\"rf\"+str(rf_pred[:,0]))\n",
    "print(grid_search.score(X_test, y_test))\n",
    "print(\"LogLoss = \"+str(log_loss(y_test, rf_pred)))\n",
    "print(confusion_matrix(y_test, rf_pred2))\n",
    "print(classification_report(y_test, rf_pred2))\n",
    "\n",
    "rf_pred_prob = grid_search.predict_proba(X_test)[:,1]\n",
    "rf_fpr, rf_tpr, thresholds = roc_curve(y_test, rf_pred_prob)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(rf_fpr, rf_tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "docTermMatrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_test = svd.fit_transform(docTermMatrix10[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=100, n_iter=7, random_state=42)\n",
    "X = svd.fit_transform(docTermMatrix[0]) \n",
    "clf = svm.SVC(decision_function_shape='ovo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clf.fit(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clf.score(X_test[:5000],y[:5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Set our parameters for xgboost\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 4\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=50, verbose_eval=10)\n",
    "\n",
    "#xgb.fit(X_train, y_train)\n",
    "\n",
    "d_test = xgb.DMatrix(X_test)\n",
    "p_test = bst.predict(d_test)\n",
    "\n",
    "#xgb_pred = xgb.predict(X_test)\n",
    "\n",
    "print(\"LogLoss = \" + str(log_loss(y_test, p_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
